---
title: "Uploading a Build"
description: "Upload a new build to Buildstash"
---

This guide covers uploading via single or multipart upload flows. Multipart is required for build files above 5GB, and optional for files smaller than this.

### Authentication

Note, a Buildstash app-level API key is required for uploading builds.

All **/upload** endpoints expect this API token in in the Authorization header as a Bearer token with each request.

#### Request to start

<Steps>
  <Step title="Request to start upload" icon="chevron-right" stepNumber={1}>
    Use [/upload/request](/api-reference/endpoint/upload/request) endpoint to pass in all key build information, request to begin the upload process, and receive a presigned URL for uploading the build file to.

    <Card title="/upload/request" icon="terminal" horizontal href="/api-reference/endpoint/upload/request">
      
    </Card>
  </Step>
</Steps>

#### Upload - Single part

<Steps>
  <Step title="Upload file to presigned URL" icon="chevron-right" stepNumber={2}>
    Upload the file

    <CodeGroup dropdown expandable>

    ```bash Request
      # Assuming you've parsed the values from previous `uploadRequest.data` into environment variables
    curl -X PUT "$PRIMARY_FILE_URL" \
    --upload-file "$PRIMARY_FILE_PATH" \
    -H "Content-Type: $PRIMARY_FILE_CONTENT_TYPE" \
    -H "Content-Length: $PRIMARY_FILE_CONTENT_LENGTH" \
    -H "Content-Disposition: $PRIMARY_FILE_CONTENT_DISPOSITION" \
    -H "x-amz-acl: private"
    ```

    
    ```javascript Request
    // uploadRequest made as in above step
    const { pending_upload_id, primary_file, expansion_files } = uploadRequest.data;
    
    await axios.put(
      primary_file.presigned_data.url,
      fs.createReadStream(primaryFilePath),
      {
        headers: {
          'Content-Type': primary_file.presigned_data.headers['Content-Type'],
          'Content-Length': primary_file.presigned_data.headers['Content-Length'],
          'Content-Disposition': primary_file.presigned_data.headers['Content-Disposition'],
          'x-amz-acl': 'private'
        },
        maxBodyLength: Infinity
      }
    );
    ```

    
    ```python Request
    # uploadRequest made as in above step
    primary_file = uploadRequest["data"]["primary_file"]
    primaryFilePath = "path/to/your/file.txt"
    
    with open(primaryFilePath, "rb") as f:
        headers = {
            "Content-Type": primary_file["presigned_data"]["headers"]["Content-Type"],
            "Content-Length": primary_file["presigned_data"]["headers"]["Content-Length"],
            "Content-Disposition": primary_file["presigned_data"]["headers"]["Content-Disposition"],
            "x-amz-acl": "private"
        }
        response = requests.put(primary_file["presigned_data"]["url"], data=f, headers=headers)
    ```

    
    ```php Request
    // uploadRequest made as in above step
    $primary_file = $uploadRequest["data"]["primary_file"];
    $primaryFilePath = "path/to/your/file.txt";
    
    $file = fopen($primaryFilePath, "rb");
    $ch = curl_init($primary_file["presigned_data"]["url"]);
    curl_setopt($ch, CURLOPT_PUT, true);
    curl_setopt($ch, CURLOPT_INFILE, $file);
    curl_setopt($ch, CURLOPT_INFILESIZE, filesize($primaryFilePath));
    curl_setopt($ch, CURLOPT_HTTPHEADER, [
        "Content-Type: " . $primary_file["presigned_data"]["headers"]["Content-Type"],
        "Content-Length: " . $primary_file["presigned_data"]["headers"]["Content-Length"],
        "Content-Disposition: " . $primary_file["presigned_data"]["headers"]["Content-Disposition"],
        "x-amz-acl: private"
    ]);
    
    curl_exec($ch);
    curl_close($ch);
    fclose($file);
    ```

    
    ```go Request
    // uploadRequest made as in above step
    
    primaryFilePath := "path/to/your/file.txt"
    data := uploadRequest["data"].(map[string]interface{})
    primary := data["primary_file"].(map[string]interface{})
    presigned := primary["presigned_data"].(map[string]interface{})
    headers := presigned["headers"].(map[string]string)
    
    file, _ := os.Open(primaryFilePath)
    defer file.Close()
    
    req, _ := http.NewRequest("PUT", presigned["url"].(string), file)
    req.Header.Set("Content-Type", headers["Content-Type"])
    req.Header.Set("Content-Length", headers["Content-Length"])
    req.Header.Set("Content-Disposition", headers["Content-Disposition"])
    req.Header.Set("x-amz-acl", "private")
    
    client := &http.Client{}
    resp, _ := client.Do(req)
    defer resp.Body.Close()
    ```

    
    ```java Request
    // uploadRequest made as in above step
    
    String primaryFilePath = "path/to/your/file.txt";
    File file = new File(primaryFilePath);
    
    Map<String, Object> data = (Map<String, Object>) uploadRequest.get("data");
    Map<String, Object> primaryFile = (Map<String, Object>) data.get("primary_file");
    Map<String, Object> presigned = (Map<String, Object>) primaryFile.get("presigned_data");
    Map<String, String> headers = (Map<String, String>) presigned.get("headers");
    
    HttpURLConnection connection = (HttpURLConnection) new URL((String) presigned.get("url")).openConnection();
    connection.setDoOutput(true);
    connection.setRequestMethod("PUT");
    
    for (Map.Entry<String, String> header : headers.entrySet())
        connection.setRequestProperty(header.getKey(), header.getValue());
    connection.setRequestProperty("x-amz-acl", "private");
    
    try (OutputStream out = connection.getOutputStream();
         FileInputStream in = new FileInputStream(file)) {
        byte[] buffer = new byte[8192];
        int len;
        while ((len = in.read(buffer)) != -1)
            out.write(buffer, 0, len);
    }
    
    System.out.println("Response: " + connection.getResponseCode());
    ```

    
    ```csharp Request
    // uploadRequest made as in above step
    
    var primaryFilePath = "path/to/your/file.txt";
    var primary = uploadRequest.data.primary_file;
    var headers = primary.presigned_data.headers;
    
    using var stream = File.OpenRead(primaryFilePath);
    
    var request = new HttpRequestMessage(HttpMethod.Put, primary.presigned_data.url)
    {
        Content = new StreamContent(stream)
    };
    
    request.Content.Headers.ContentType = new MediaTypeHeaderValue(headers["Content-Type"]);
    request.Content.Headers.ContentLength = long.Parse(headers["Content-Length"]);
    request.Headers.Add("Content-Disposition", headers["Content-Disposition"]);
    request.Headers.Add("x-amz-acl", "private");
    
    var client = new HttpClient();
    var response = await client.SendAsync(request);
    ```

    
    ```ruby Request
    # uploadRequest made as in above step
    
    primary_file = uploadRequest[:data][:primary_file]
    primaryFilePath = "path/to/your/file.txt"
    uri = URI.parse(primary_file[:presigned_data][:url])
    
    request = Net::HTTP::Put.new(uri)
    headers = primary_file[:presigned_data][:headers]
    request["Content-Type"] = headers["Content-Type"]
    request["Content-Length"] = headers["Content-Length"]
    request["Content-Disposition"] = headers["Content-Disposition"]
    request["x-amz-acl"] = "private"
    request.body = File.read(primaryFilePath)
    
    response = Net::HTTP.start(uri.hostname, uri.port, use_ssl: true) do |http|
      http.request(request)
    end
    
    puts response.code
    ```

    </CodeGroup>

    If the upload to S3 was successful, you've receive a response like:

    <CodeGroup>

    ```bash Response
    /HTTP/1.1 200 OK
    Date: Wed, 30 Jul 2025 20:48:28 GMT
    Content-Type: text/plain;charset=UTF-8
    Content-Length: 0
    Connection: keep-alive
    ETag: "a4d9c87f3e6b41c29fb2e7a8d1035b6e"
    x-amz-checksum-crc64nvme: H1KdR+1sY8c=
    x-amz-version-id: 93cf1f4e9e3d4ac2a8a4f929b03bd7a9
    Server: cloudflare
    CF-RAY: 9802ce3d5f6a1234-LAX
    ```

    </CodeGroup>

    Note the response may vary slightly depending on your S3 provider, for example AWS S3, Cloudflare R2, etc.
  </Step>
</Steps>

#### Upload - Multipart

<Steps>
  <Step title="Request to start upload" icon="chevron-right" stepNumber={1}>
    Use [/upload/request/multipart](/api-reference/endpoint/upload/request/multipart) endpoint to request the presigned URL for each part.

    <Card title="/upload/request/multipart" icon="terminal" horizontal href="/api-reference/endpoint/upload/request/multipart">
      
    </Card>
  </Step>
  <Step title="Upload part" icon="chevron-right">
    For multipart uploads, once you have the presigned URL for each part, you'll need to upload that part, store returned ETag, and upload the next part, until all are complete. The provided examples demonstrate a relatively simple approach to this. Note more advanced approaches are possible including parallel uploads for efficiency.

    <CodeGroup dropdown expandable>

    ```bash Request
     # Set vars
FILE="yourfile.bin"
SIZE=$(stat -c %s "$FILE")
CHUNK_SIZE_MB=5
CHUNK_SIZE=$((CHUNK_SIZE_MB * 1024 * 1024))
API_KEY="your_api_key"
PENDING_ID="your_pending_upload_id"

# Upload loop
for i in $(seq 0 $(( (SIZE + CHUNK_SIZE - 1) / CHUNK_SIZE - 1))); do
  OFFSET=$((i * CHUNK_SIZE))
  PART_NUM=$((i + 1))
  LEN=$((CHUNK_SIZE))
  [ $((OFFSET + LEN)) -gt $SIZE ] && LEN=$((SIZE - OFFSET))

  echo "Uploading part $PART_NUM"

  PRESIGNED=$(curl -s -X POST https://app.buildstash.com/api/v1/upload/request/multipart \
    -H "Authorization: Bearer $API_KEY" \
    -H "Content-Type: application/json" \
    -d "{\"pending_upload_id\":\"$PENDING_ID\",\"part_number\":$PART_NUM,\"content_length\":$LEN}")

  URL=$(echo $PRESIGNED | jq -r .part_presigned_url)

  dd if="$FILE" bs=1 skip=$OFFSET count=$LEN 2>/dev/null | \
    curl -X PUT "$URL" -H "Content-Length: $LEN" -H "Content-Type: application/octet-stream" --data-binary @-
done
    ```

    
    ```javascript Request
    // Pass in values from initial upload request, and get parts etags back to use for verifying upload completion later
    async function uploadChunkedFile({
  filePath,
  filesize,
  pendingUploadId,
  chunkedNumberParts,
  chunkedPartSizeMb,
  apiKey,
}) {
  // Read the file at filePath into a buffer
  const chunkSize = chunkedPartSizeMb * 1024 * 1024;
  const parts = [];

  // Loop through each part, get presigned URL, and upload it
  for (let i = 0; i < chunkedNumberParts; i++) {

    const chunkStart = i * chunkSize;
    const chunkEnd = Math.min((i + 1) * chunkSize - 1, filesize - 1);
    let chunkStream = fs.createReadStream(filePath, { start: chunkStart, end: chunkEnd });

    const contentLength = chunkEnd - chunkStart + 1;

    const partNumber = i + 1;

    core.info('Uploading chunked upload, part: ' + partNumber + ' of ' + chunkedNumberParts);

    // Request presigned URL for this part
    const presignedResp = await axios.post(
      'https://app.buildstash.com/api/v1/upload/request/multipart',
      {
        pending_upload_id: pendingUploadId,
        part_number: partNumber,
        content_length: contentLength
      },
      {
        headers: {
          'Authorization': Bearer ${apiKey},
          'Content-Type': 'application/json'
        }
      }
    );

    // Get presigned URL for this part from response
    const presignedUrl = presignedResp.data.part_presigned_url;

    // Upload chunk via presigned URL (on failure retry part once before error)
    let uploadResponse;
    let uploadError;
    // Attach error handler to the stream
    chunkStream.on('error', (err) => {
      core.error(File stream error for part ${partNumber}: ${err.message});
    });
    for (let attempt = 1; attempt <= 2; attempt++) {
      try {
        uploadResponse = await axios.put(
          presignedUrl,
          chunkStream,
          {
            headers: {
              'Content-Type': 'application/octet-stream',
              'Content-Length': contentLength
            },
            maxBodyLength: Infinity
          }
        );
        uploadError = null;
        break; // Success, exit retry loop
      } catch (err) {
        uploadError = err;
        // Log more error details
        if (err.response) {
          core.error(Chunk upload for part ${partNumber} failed (attempt ${attempt}): ${err.message}, status: ${err.response.status}, data: ${JSON.stringify(err.response.data)});
        } else {
          core.error(Chunk upload for part ${partNumber} failed (attempt ${attempt}): ${err.message});
        }
        if (attempt === 1) {
          // Re-create the stream for retry
          chunkStream.destroy();
        }
      }
      // If retrying, re-create the stream
      if (attempt === 1 && uploadError) {
        // Wait a short delay before retrying (optional, can be omitted or tuned)
        await new Promise(res => setTimeout(res, 500));
        // Re-create the stream for the retry
        chunkStream = fs.createReadStream(filePath, { start: chunkStart, end: chunkEnd });
        chunkStream.on('error', (err) => {
          core.error(File stream error for part ${partNumber} (retry): ${err.message});
        });
      }
    }
    if (uploadError) {
      throw uploadError;
    }
    // Check for ETag presence
    if (!uploadResponse.headers.etag) {
      core.warning(No ETag returned for part ${partNumber}. Response headers: ${JSON.stringify(uploadResponse.headers)});
    }

    // Add part to parts array
    parts.push({
      PartNumber: partNumber,
      ETag: uploadResponse.headers.etag
    });
  }

  // Return parts array
  return parts;
}
    ```

    
    ```python Request
    # Pass in values from initial upload request, and get parts etags back to use for verifying upload completion later
    ```

    
    ```php Request
    // Pass in values from initial upload request, and get parts etags back to use for verifying upload completion later
    ```

    
    ```go Request
    // Pass in values from initial upload request, and get parts etags back to use for verifying upload completion later
    ```

    
    ```java Request
    // Pass in values from initial upload request, and get parts etags back to use for verifying upload completion later
    ```

    
    ```csharp Request
    // Pass in values from initial upload request, and get parts etags back to use for verifying upload completion later
    ```

    
    ```ruby Request
    # Pass in values from initial upload request, and get parts etags back to use for verifying upload completion later
    ```

    </CodeGroup>

    If the upload to S3 was successful, you've receive a response like:

    <CodeGroup>

    ```bash Response
    /HTTP/1.1 200 OK
    Date: Wed, 30 Jul 2025 20:48:28 GMT
    Content-Type: text/plain;charset=UTF-8
    Content-Length: 0
    Connection: keep-alive
    ETag: "a4d9c87f3e6b41c29fb2e7a8d1035b6e"
    x-amz-checksum-crc64nvme: H1KdR+1sY8c=
    x-amz-version-id: 93cf1f4e9e3d4ac2a8a4f929b03bd7a9
    Server: cloudflare
    CF-RAY: 9802ce3d5f6a1234-LAX
    ```

    </CodeGroup>
    
  </Step>
</Steps>

#### Expansion upload

You may also optionally

<Steps>
  <Step title="Request to start upload" icon="chevron-right" stepNumber={1}>
    Use [/upload/request](/api-reference/endpoint/upload/request) endpoint to pass in all key build information, request to begin the upload process, and receive a presigned URL for uploading the build file to.

    <Card title="/upload/request" icon="terminal" horizontal href="/api-reference/endpoint/upload/request">
      
    </Card>
  </Step>
</Steps>

#### Verify and complete

<Steps>
  <Step title="Verify and complete upload" icon="chevron-right" stepNumber={4}>
    Use [/upload/verify](/api-reference/endpoint/upload/verify) endpoint to inform Buildstash the upload is complete, validate all parts have successfully uploaded, and complete the upload flow.

    <Card title="/upload/verify" icon="terminal" horizontal href="/upload/verify">
      
    </Card>
  </Step>
</Steps>

### Best practices

1. **Retry Logic**: Implement retry logic for failed uploads
2. **Progress Tracking**: Track upload progress for large files
3. **Parallel Uploads**: Upload parts in parallel for better performance
4. **Validation**: Verify file integrity after upload
5. **Cleanup**: Handle cleanup on upload failure

### Provided integrations

<Card title="CI Integrations" href="https://support.buildstash.com/docs">
  We provide a number of off-the-shelf integrations for uploading builds to Buildstash via widely used CI platforms - like GitHub Actions and Azure Pipelines.
</Card>